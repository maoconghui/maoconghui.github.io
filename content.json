{"meta":{"title":"MaoConghui","subtitle":null,"description":"Better late than never","author":"MaoConghui","url":"http://yoursite.com"},"posts":[{"title":"Hadoop集群搭建之HBase安装","date":"2019-03-22T04:00:00.000Z","path":"2019/03/22/Hadoop集群搭建之HBase安装 /","text":"一 集群规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager、Zookeeper、HMaster hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager、Zookeeper、HRegionServer hadoop3 192.168.159.131 DataNode、NodeManager、Zookeeper、HRegionServer 二 配置HBase信息将压缩包hbase-1.2.4.tar.gz放到/home/mch目录下，并解压 # tar –zxvf hbase-1.2.4.tar.gz 配置hbase环境变量 # vim /etc/profile 添加内容如下： export HBASE_HOME=/opt/hbase-1.2.4 export PATH=$PATH:$HBASE_HOME/bin 在/hbase-1.2.4/conf目录下，修改hbase-env.sh # vim hbase-env.sh 修改内容如下： export JAVA_HOME=/home/mch/java/jdk1.8 export HBASE_LOG_DIR=/home/mch/hbase-1.2.4/logs export HBASE_MANAGES_ZK=false export HBASE_PID_DIR=/home/mch/hbase-1.2.4/pids 同时，在/home/mch/hbase-1.2.4/目录下新建logs和pids目录 # mkdir logs pids data 在/hbase-1.2.4/conf目录下修改regionserver # vim regionservers 将里面的localhost删除并添加如下内容： hadoop1 hadoop2 hadoop3 在/hbase-1.2.4/conf目录下修改hbase-site.xml # vim hbase-site.xml 修改内容如下： &lt;configuration&gt; &lt;!-- 指定RegionServer的共享目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HBase的运行模式为分布式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Zookeeper集群的地址列表 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HBase的主节点地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:60000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定快照的存储位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/zookeeper-3.4.9/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定客户端连接的端口 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 将hbase-1.2.4目录拷贝给其他节点 # scp –r /opt/hbase-1.2.4 slave1:/opt # scp –r /opt/hbase-1.2.4 slave2:/opt 三 启动HBase# cd /opt/hbase-1.2.4/bin # ./start-hbase.sh 注意： 各个服务启动情况：hadoop-&gt;zookeeper-&gt;hbase （1）启动Hadoop只需在主节点上执行# ./start-all.sh start （2）启动Zookeeper需要在各个节点上执行# ./zkServer.sh start （3）启动HBase只需在主节点上执行# ./start-hbase.sh start 四 查看HBase是否安装成功# hbase shell # list 五 遇到的问题1.Main进程 在hbase shell下，使用ctrl+z退出后查看进程会多出一个Main进程，ctrl+z退出几次，就会有几个Main进程，所以最好使用quit退出。","permalink":"http://yoursite.com/2019/03/22/Hadoop集群搭建之HBase安装 /","categories":[],"tags":[{"name":"Hadoop HBase","slug":"Hadoop-HBase","permalink":"http://yoursite.com/tags/Hadoop-HBase/"}]},{"title":"Hadoop集群搭建之Zookeeper安装","date":"2019-03-20T09:20:00.000Z","path":"2019/03/20/Hadoop集群搭建之Zookeeper安装/","text":"一 集群规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager、Zookeeper hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager、Zookeeper hadoop3 192.168.159.131 DataNode、NodeManager、Zookeeper 二 配置Zookeeper信息将zookeeper压缩包放在/home/mch目录下，并进行解压。 # tar -zxvf zookeeper-3.4.9.tar.gz 配置zookeeper环境变量 # vi /etc/profile 增加如下信息： export ZOOKEEPER_HOME=/home/mch/zookeeper-3.4.9 export PATH=$ZOOKEEPER_HOME/bin:$PATH 然后生效该文件 # source /etc/profile 在zookeeper-3.4.9中创建data和logs文件 # mkdir data logs 在data文件中新建myid文件，并编辑为1. # vi myid 进入到/home/mch/zookeeper-3.4.9/conf目录下，将zoo_sample.cfg重命名为zoo.cfg # mv zoo_sample.cfg zoo.cfg 编辑zoo.cfg信息 # vi zoo.cfg 修改内容如下： dataDir=/home/mch/zookeeper-3.4.9/data dataLogDir=/home/mch/zookeeper-3.4.9/logs server.1=192.168.159.129:2888:3888 #server.1、server.2、server.3分别对应myid下的1、2、3 server.2=192.168.159.130:2888:3888 server.3=192.168.159.131:2888:3888 将hadoop1节点下的zookeeper-3.4.9目录拷贝到hadoop2和hadoop3中 # scp -r zookeeper-3.4.9 hadoop2:/home/mch # scp -r zookeeper-3.4.9 hadoop3:/home/mch 修改hadoop2和hadoop3中的myid文件内容分别为2和3。 三 启动Zookeeper在三个节点上分别启动zookeeper服务 # cd /home/mch/zookeeper-3.4.9/bin # ./zkServer.sh start 查看三个节点中是否都有Zookeeper进程 # jps 四 查看zookeeper状态# ./zkServer.sh status 根据paxos选举规则，可以得到一个master，两个slave 五 遇到的问题—–Zookeeper状态显示Error contacting service. It is probably not running.—– 查看配置是否出错，myid是否写错。 —–出现Error Contacting Service,It is probably not running.—– Zookeeper开启的台数没有大于一半，","permalink":"http://yoursite.com/2019/03/20/Hadoop集群搭建之Zookeeper安装/","categories":[],"tags":[{"name":"Hadoop Zookeeper","slug":"Hadoop-Zookeeper","permalink":"http://yoursite.com/tags/Hadoop-Zookeeper/"}]},{"title":"Hadoop伪分布式集群搭建","date":"2019-03-20T06:06:00.000Z","path":"2019/03/20/Hadoop伪分布式集群搭建/","text":"一 配置环境及规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager hadoop3 192.168.159.131 DataNode、NodeManager 二 环境搭建及配置1 Linux环境搭建下载VMWare Workstation，搭建Linux环境（典型），安装CentOS7系统，并命名为hadoop1。 注意：安装镜像文件时若显示无法检测到操作系统，则选择稍后安装，下一步之后点击自定义硬件，在新CD/DVD中添加镜像文件。 然后克隆两个虚拟机，在虚拟机-管理-克隆-创建完整克隆，并重命名hadoop2，hadoop3。 注意：克隆的节点与主节点的ip地址是一样的，所以为了避免地址冲突，需要对克隆后的节点进行网络配置。 2 网络配置对三个节点进行网络配置，进入/etc/sysconfig/network-scripts目录,修改第一个形如ifcfg-ens的文件。 # cd /etc/sysconfig/network-scripts # vim ifcfg-ens33 修改并添加信息如下： ONBOOT=yes BOOTPROTO=static IPADDR=192.168.159.129 #根据网关自行调整【ip add】 PREFIX=255.255.255.0 #注意需要使用PREFIX GATEWAY=192.168.159.2 DNS1=114.114.114.114 DNS2=202.96.134.133 然后重启网络： # systemctl restart network.service 测试网络： # ping www.baidu.com 在hadoop2和hadoop3的ifcfg-ens文件中配置其ip。 # vi ifcfg-ens33 设置集群的主机名 # hostnamectl set-hostname hadoop2 设置如下： 192.168.159.129 hadoop1 192.168.159.130 hadoop2 192.168.159.131 hadoop3 3 关闭防火墙端口绑定了防火墙，所以需要关闭（三个节点都关闭）。 # systemctl stop firewalld.service #关闭防火墙 # systemctl disable firewalld.service #禁止开机启动 # vim /etc/selinux/config #设置selinux = disabled 4 配置SSH免密登录当搭建linux集群环境的时候，从一个节点登录到另一个节点，通过ssh方式，每次登录跳转都需要输入密码，这样造成非常不便，其实可以通过配置SSH互信，来实现集群节点间的免密码登录跳转。 公钥认证的基本思想： 对信息的加密和解密采用不同的key，这对key分别称作private key和public key，其中，public key存放在欲登录的服务器上，而private key为特定的客户机所持有。当客户机向服务器发出建立安全连接的请求时，首先发送自己的public key，如果这个public key是被服务器所允许的，服务器就发送一个经过public key加密的随机数据给客户机，这个数据只能通过private key解密，客户机将解密后的信息发还给服务器，服务器验证正确后即确认客户机是可信任的，从而建立起一条安全的信息通道。通过这种方式，客户机不需要向外发送自己的身份标志“private key”即可达到校验的目的，并且private key是不能通过public key反向推断出来的。这避免了网络窃听可能造成的密码泄露。客户机需要小心的保存自己的private key，以免被其他人窃取，一旦这样的事情发生，就需要各服务器更换受信的public key列表。 简言之，当我们在搭建集群过程中，在集群中机器间相互通信的时候，会不断的提示你输入通信机器的密码，然而当我们建立起集群间的ssh互信后，任意两台机器之间可以无密码的方便通信。 4.1 创建公钥秘钥# mkdir ~/.ssh #创建.ssh目录（如果目录存在，就不必要创建） # chmod 700 ~/.ssh #更改.ssh权限 # cd ~/.ssh #进入.ssh目录 # ssh-keygen -t rsa 以上操作三个节点都要设置 4.2 修改hosts文件# vi /etc/hosts 添加如下信息： 192.168.159.129 hadoop1 192.168.159.130 hadoop2 192.168.159.131 hadoop3 以上操作只在主节点上设置 4.3 整合公钥文件将hadoop1中的id_rsa.pub拷贝到其他服务器中 # cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # ssh hadoop2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # ssh hadoop3 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # chmod 600 ~/.ssh/authorized_keys 4.4 分发整合后的公钥文件将整合后的公钥文件从hadoop1分别复制到hadoop2和hadoop3上 # scp ~/.ssh/authorized_keys hadoop2:~/.ssh/ # scp ~/.ssh/authorized_keys hadoop3:~/.ssh/ 5 分发hosts文件我们需要将我们在hadoop1节点上配置的hosts文件分发到所有的节点上，在hadoop1节点上输入如下命令： # scp /etc/hosts hadoop2:/etc # scp /etc/hosts hadoop3:/etc 6 测试SSH互信（三个节点都设置）在各个节点上运行以下命令，若不需要输入密码就显示系统当前日期，就说明SSH互信已经配置成功了，在hadoop1节点上输入以下命令： # ssh hadoop2 date # ssh hadoop3 date 然后分别在hadoop2和hadoop3节点分别输入以下命令： # ssh hadoop1 date # ssh hadoop3 date # ssh hadoop1 date # ssh hadoop2 date 7 设置时间同步利用NTP将三个节点的时间设置为同步（一个服务端，两个客户端） 如果一个集群中，时间相差很大，那么会出现很多诡异的问题。所以，设置服务器之间时间同步是必不可少的！ 7.1 准备工作首先安装ntp # yum install ntp 安装完毕之后，启动服务 # systemctl start ntpd.service 设置开机自启动 # systemctl enable ntpd.service 以上操作三个节点都要设置 7.2 NTP服务端设置第一台服务器192.168.159.129，即hadoop1，作为ntpserver，将他设置为同步外网时间（ntpd服务开启默认就同步了），但是得设置允许内网网段可以连接它，将它作为内网的时间同步服务器。 修改/etc/ntp.conf文件 # vim /etc/ntp.conf 添加如下信息，表示允许159网段来同步此服务器 restrict 192.168.159.0 mask 255.255.255.0 设置后，重启ntpd服务，用ntpstat来检查效果 # systemctl restart ntpd # ntpstat 7.3 NTP客户端设置（在hadoop2和hadoop3两个节点上设置）第二、三台服务器192.168.159.130，192.168.159.131，即hadoop2和hadoop3，作为ntpclient，将他设置为同步上面的ntpserver，分别在两个节点上做如下配置。 修改/etc/ntp.conf文件 # vim /etc/ntp.conf 注释掉外网时间服务器，添加本地服务器即可 server 192.168.159.130 #添加此行（131服务器上则添加server 192.168.159.131） # server 0.centos.pool.ntp.org iburst #以下四行注释掉 # server 1.centos.pool.ntp.org iburst # server 2.centos.pool.ntp.org iburst # server 3.centos.pool.ntp.org iburst 设置后，重启ntpd服务，用ntpstat来检查效果 # systemctl restart ntpd # ntpstat 8 安装jdk下载想安装的jdk版本之后，将压缩包放到/home/mch目录，解压缩后，更名为jdk1.8 # cd /home/mch #进入到压缩包所在目录下 # tar -xzvf jdk-8u152-linux-x64.tar.gz #解压Linux版本的jdk解压包 # mv jdk1.8.0_111 jdk1.8 #重命名文件夹 配置环境变量，修改/etc/profile文件 # vim /etc/profile 在最下方添加三行配置 export JAVA_HOME=/home/mch/jdk1.8 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 使修改内容生效 # source /etc/profile 以上操作三个节点都要设置 9 安装mysql官网下载安装mysql-server # wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm # rpm -ivh mysql-community-release-el7-5.noarch.rpm # yum install mysql-community-server 安装成功后重启mysql服务 # systemctl restart mysqld.service # systemctl enable mysqld.service #设置mysql允许开机自启动 mysql只需要在主节点安装。 三 安装配置Hadoop2.7.2集群1 安装hadoop将下载的hadoop2.6.0压缩包，上传到主节点的目录下 # cd /home/mch 进行解压缩 # tar -xzvf hadoop-2.6.0-x64.tar.gz 修改目录为hadoop2.6.0 # mv hadoop-2.6.0 hadoop2.6.0 修改配置文件，加入hadoop的环境变量 # vim /etc/profile 增加内容如下 export JAVA_HOME=/home/mch/jdk1.8 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin export HADOOP_HOME=/home/mch/hadoop2.6.0 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 新建所需目录 # mkdir /home/mch/hadoop2.6.0/tmp # mkdir /home/mch/hadoop2.6.0/var #mkdir /home/mch/hadoop2.6.0/dfs # mkdir /home/mch/hadoop2.6.0/dfs/name # mkdir /home/mch/hadoop2.6.0/dfs/data 2 修改hadoop-env.sh文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim hadoop-env.sh 将:export JAVA_HOME=${JAVA_HOME} 修改为：export JAVA_HOME=/home/mch/jdk1.8 #修改为jdk目录 3 修改slaves文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim slaves 增加如下内容 hadoop2 hadoop3 此时hadoop1作为主节点，以及主备节点，管理节点，而同时hadoop1，hadoop2，hadoop3都作为数据节点。 4 修改core-site.xml文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim core-site.xml 修改内容如下 &lt;configuration&gt; &lt;!-- 指定hadoop的临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定集群中NameNode结点的URI(包括协议、主机名称、端口号) --&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 5 修改hdfs-site.xml文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim hdfs-site.xml 修改内容如下 &lt;configuration&gt; &lt;!-- 设置NameNode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;hadoop1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定SecondaryNameNode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop2:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode结点存储hadoop文件系统信息的本地系统路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/dfs/name&lt;/value&gt; &lt;description&gt;Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定DataNode结点被指定要存储数据的本地文件系统路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/dfs/data&lt;/value&gt; &lt;description&gt;Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定系统文件块的数据备份个数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭hadoop目录的权限 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;need not permissions&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 6 修改mapred-site.xml文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # cp mapred-site.xml.template mapred-site.xml # vim hdfs-site.xml 修改内容如下 &lt;configuration&gt; &lt;!-- 指定JobTracker的主机和端口 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.tracker&lt;/name&gt; &lt;value&gt;hadoop1:49001&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MapReduce框架为Yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 7 修改yarn-site.xml文件# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim yarn-site.xml 修改内容如下 &lt;configuration&gt; &lt;!-- 指定ResourceManager所在节点的主机名 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ResourceManager提供给客户端访问的地址 --&gt; &lt;property&gt; &lt;description&gt;The address of the applications manager interface in the RM.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;${yarn.resourcemanager.hostname}:8032&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NodeManager执行的服务 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定向ResourceManager申请的最大内存量 --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;12288&lt;/value&gt; &lt;discription&gt;每个节点可用内存,单位MB,默认8182MB&lt;/discription&gt; &lt;/property&gt; &lt;/configuration&gt; 在主节点上配置好hadoop包后，同步到另外两个节点，配置不用修改，三个节点的配置都一样！ 注意：在实际应用中，应将NameNode和SecondaryNameNode配置在不同的节点上，否则当NameNode宕机时，SecondaryNameNode也会故障。 四 hadoop集群的初始化和启动1 初始化hadoop集群# cd /home/mch/hadoop2.7.2/bin # ./hadoop namenode -format 格式化成功后，可以在看到在/home/mch/hadoop2.7.2/dfs/name/目录多了一个current目录，而且该目录内有4个文件。 2 启动dfs# cd /home/mch/hadoop2.7.2/sbin # ./start-dfs.sh 在三个节点上分别输入一下命令，查看三个节点上开启的进程 # jps 可以看到在hadoop1中有NameNode、DataNode，在hadoop2中有SecondaryNameNode、DataNode，在hadoop3中有DataNode。 3 启动yarn# cd /home/mch/hadoop2.7.2/sbin # ./start-yarn.sh 在三个节点上分别输入一下命令，查看三个节点上开启的进程 # jps 可以看到此时hadoop1中多了ResourceManager和NodeManager，hadoop2和hadoop3中分别多了NodeManager。 五 集群验证可通过命令jps来查看集群的进程 也可通过网页来查看 http://192.168.159.129:50070 http://192.168.159.129:8088 六 遇到的问题（不定时更新）—–DataNode启动失败—– 有两个原因： 1、DataNode和NameNode的ClusterID不一致。是由于多次初始化namenode（即./hdfs namenode -format）造成的，每一次format，NameNode都会生成新的ClusterID，而DataNode还是保持原来的ClusterID。 2、将hadoop1中的配置文件拷贝到其他节点中，文件权限未开启。 原因1解决办法： 1.复制hadoop2.7.2/dfs/name/current/VERSION文件中namenode的clusterID. 2.用该clusterID把所有datanode节点机器中hadoop2.7.2/dfs/data/current/VERSION中的clusterID替换掉 原因2解决办法： # chmod 777 -R /home/mch/hadoop2.6.0","permalink":"http://yoursite.com/2019/03/20/Hadoop伪分布式集群搭建/","categories":[],"tags":[{"name":"Hadoop NameNode DataNode","slug":"Hadoop-NameNode-DataNode","permalink":"http://yoursite.com/tags/Hadoop-NameNode-DataNode/"}]},{"title":"Ganglia监控程序安装","date":"2019-03-02T10:09:00.000Z","path":"2019/03/02/Ganglia监控程序安装/","text":"Best or Nothing 一 配置环境及规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmetad、gmond、gweb hadoop2 192.168.159.130 NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmond hadoop3 192.168.159.131 DataNode、Zookeeper、NodeManager、HRegionServer、gmond 二 安装过程1 下载Ganglia及其依赖包在集群的三个节点上分别下载Ganglia及其依赖包，通过yum只下载不安装，需要安装一个yum的插件。 # yum install yum-downloadonly 安装完毕之后下载需要安装的rpm包以及依赖 # yum install rrdtool httpd php ganglia* libconfuse --downloadonly --downloaddir=/home 2 安装Ganglia到监控端在hadoop1上安装ganglia-gmetad、ganglia-web、php、httpd组件 # yum install rrdtool httpd php ganglia-gmetad ganglia-gmond ganglia-web 注意：有监控端才需要安装ganglia-metad、ganglia-web、php和httpd，其他机器不用安装 3 安装Ganglia到被监控端在hadoop2和hadoop2上安装ganglia-gmond组件 # yum install ganglia-gmond 注意：被监控端只需要安装ganglia-gmond，不需要安装ganglia的其他组件。 三 配置过程1 配置监控端的ganglia-metad和ganglia-web监控端端配置文件/etc/ganglia/gmetad.conf：主要是配置data_source参数。它设定了被监控端服务器的地址及端口，可以指定多个被监控端服务器： data_source &quot;hadoop&quot; 15 192.168.159.129:8649 192.168.159.130:8649 192.168.159.131:8649 将/usr/share/ganglia目录软链接到/var/www/html目录下 # ln -s /usr/share/ganglia /var/www/html 修改web前端配置文件/var/www/html/ganglia/conf.php，指定gmetad中存储rrd图形的目录，以及rrdtool的位置： $gmetad_root = &quot;/var/lib/ganglia&quot;; $rrds = &quot;$gmetad_root/rrds&quot;; define(&quot;RRDTOOL&quot;, &quot;/usr/bin/rrdtool&quot;); 修改/etc/httpd/conf.d/ganglia.conf 2 配置监控机器的ganglia-gmondgmond.conf包括了几个部分：globals、cluster、udp_send_channel、udp_recv_channel等，如果只是想要Ganglia简单地运行，两个操作就可以了，两个操作都是在cluster配置段中进行修改： 修改cluster的名字，命名成“hadoop”与data_source一致 cluster { name =&quot;hadoop&quot; # 和data_source中的名字一致 owner = &quot;unspecified&quot; latlong = &quot;unspecified&quot; url = &quot;unspecified&quot; } 修改接收和发送的地址，如下图所示。 udp_send_channel { #bind_hostname = yes # Highly recommended, soon to be default. #This option tells gmond to use a source address #that resolves to the machine&apos;s hostname. Without #this, the metrics may appear to come from any #interface and the DNS names associated with #those IPs will be used to create the RRDs. #mcast_join = 239.2.11.71 host=192.168.159.129 # gmetad所在服务器的地址 port = 8649 ttl = 1 } /* You can specify as many udp_recv_channels as you like as well. */ udp_recv_channel { #mcast_join = 239.2.11.71 port = 8649 #bind = 239.2.11.71 retry_bind = true #Size of the UDP buffer. If you are handling lots of metrics you really #should bump it up to e.g. 10MB or even higher. #buffer = 10485760 } 3 配置Hadoop在Hadoop中，对Ganglia的兼容是很好的，在Hadoop的目录下/hadoop-2.7.2/etc/hadoop，我们可以找到hadoop-metrics2.properties文件，这里我们修改文件内容如下所示， 4 配置HBase在HBase的目录下/hbase-1.2.4/conf，我们可以找到hadoop-metrics2-hbase.properties文件，这里我们修改文件内容如下所示 然后将这两个文件从hadoop1复制到hadoop2和hadoop3，替换掉其原有文件。 四 开启监控程序首先开启节点的gmond，开启的顺序为先开启发送节点的gmond，再开启其他节点的gmond，即本文先开启hadoop1的gmond，再开启hadoop2和hadoop3的gmond，命令如下： # systemctl start gmond 然后开启主节点的gmetad，命令如下： # systemctl start gmetad 最后开启主节点的httpd # systemctl start httpd 五 查看状态# systemctl status gmond -l # systemctl status gmetad -l # systemctl status httpd -l 最后通过浏览http://192.168.159.129/ganglia查看监控程序。 六 遇到的问题及解决办法以下有些为在其他集群运行遇到的问题。 —–data_thread() for [hadoop] failed to contact node …—– 在查看gmetad状态时，出现如下错误 是因为节点的gmond未启动，可先查看对应节点的gmond的状态，若无错误，可能是gmond的启动顺序不当造成的，可按顺序重启gmond。 # systemctl restart gmond —–Error creating UDP server on port 8649 bind=…—– 查看gmond的状态信息，若出现如下错误 是因为在gmond.conf配置文件中，udp_recv_channel的bind地址写错了，应该将接收的bind的地址写成当前节点的地址或者直接注释掉。 —–访问ganglia页面的时候出现Forbidden You don’t have permission to access /ganglia on this server.—– 解决的方法：1、运行命令禁用SELinux：setenforce 0（查看SELinux命令：getenforce）2、运行sudo vi /etc/httpd/conf.d/ganglia.conf，将Deny from all注释掉就可以了。3、重启httpd：systemctl restart httpd —–Failed to start The Apache HTTP Server.—– 启动httpd失败 原因是端口未开启，执行 # fuser -k -n tcp 80 之后便可启动了。 —–AH00558:httpd:Could not reliably determine the server’s fully qualified domain name,using … .Set the ‘ServerName’ directive globally to suppress this message.—– 在查看http状态时出现如下信息 在/etc/httpd/conf.d/httpd.conf中加上ServerName 192.110.10.120 —–(code=exited, status=1/FAILURE)—– 在查看http状态时出现如下信息 将/etc/httpd/conf/httpd.conf 中的Listen 8080改为80","permalink":"http://yoursite.com/2019/03/02/Ganglia监控程序安装/","categories":[{"name":"大数据集群","slug":"大数据集群","permalink":"http://yoursite.com/categories/大数据集群/"}],"tags":[{"name":"Ganglia","slug":"Ganglia","permalink":"http://yoursite.com/tags/Ganglia/"},{"name":"监控","slug":"监控","permalink":"http://yoursite.com/tags/监控/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://yoursite.com/tags/Hadoop/"},{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"test","date":"2018-12-03T01:39:00.000Z","path":"2018/12/03/article/","text":"优秀的人，不是不合群，而是他们合群的人里面没有你","permalink":"http://yoursite.com/2018/12/03/article/","categories":[{"name":"test","slug":"test","permalink":"http://yoursite.com/categories/test/"}],"tags":[{"name":"test 测试","slug":"test-测试","permalink":"http://yoursite.com/tags/test-测试/"}]},{"title":"Hello World","date":"2018-11-30T11:22:52.782Z","path":"2018/11/30/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","permalink":"http://yoursite.com/2018/11/30/hello-world/","categories":[],"tags":[]}]}