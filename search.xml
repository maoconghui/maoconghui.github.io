<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hadoop集群搭建之HBase安装</title>
      <link href="/2019/03/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BHBase%E5%AE%89%E8%A3%85%20/"/>
      <url>/2019/03/22/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BHBase%E5%AE%89%E8%A3%85%20/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>HBase是一个开源的非关系型分布式数据库，它参考了谷歌的BigTable建模，实现的编程语言为Java。它是Apache软件基金会的Hadoop项目的一部分，运行于HDFS文件系统之上，为 Hadoop 提供类似于BigTable 规模的服务。因此，它可以对稀疏文件提供极高的容错率。</p><a id="more"></a><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th style="text-align:center">主机名</th><th style="text-align:center">IP</th><th style="text-align:center">集群规划</th></tr></thead><tbody><tr><td style="text-align:center">hadoop1</td><td style="text-align:center">192.168.159.129</td><td style="text-align:center">NameNode、DataNode、NodeManager、ResourceManager、Zookeeper、HMaster</td></tr><tr><td style="text-align:center">hadoop2</td><td style="text-align:center">192.168.159.130</td><td style="text-align:center">SecondaryNameNode、DataNode、NodeManager、Zookeeper、HRegionServer</td></tr><tr><td style="text-align:center">hadoop3</td><td style="text-align:center">192.168.159.131</td><td style="text-align:center">DataNode、NodeManager、Zookeeper、HRegionServer</td></tr></tbody></table><h2 id="配置HBase信息"><a href="#配置HBase信息" class="headerlink" title="配置HBase信息"></a>配置HBase信息</h2><p>将压缩包hbase-1.2.4.tar.gz放到/home/mch目录下，并解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar –zxvf  hbase-1.2.4.tar.gz</span></span><br></pre></td></tr></table></figure><p>配置hbase环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>添加内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HBASE_HOME=/opt/hbase-1.2.4</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$HBASE_HOME/bin</span><br></pre></td></tr></table></figure><p>在/hbase-1.2.4/conf目录下，修改hbase-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim hbase-env.sh</span></span><br></pre></td></tr></table></figure><p>修改内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/mch/java/jdk1.8</span><br><span class="line"></span><br><span class="line">export HBASE_LOG_DIR=/home/mch/hbase-1.2.4/logs</span><br><span class="line"></span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line"></span><br><span class="line">export HBASE_PID_DIR=/home/mch/hbase-1.2.4/pids</span><br></pre></td></tr></table></figure><p>同时，在/home/mch/hbase-1.2.4/目录下新建logs和pids目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir logs pids data</span></span><br></pre></td></tr></table></figure><p>在/hbase-1.2.4/conf目录下修改regionserver</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim regionservers</span></span><br></pre></td></tr></table></figure><p>将里面的localhost删除并添加如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop1</span><br><span class="line"></span><br><span class="line">hadoop2</span><br><span class="line"></span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p>在/hbase-1.2.4/conf目录下修改hbase-site.xml</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim hbase-site.xml</span></span><br></pre></td></tr></table></figure><p>修改内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定RegionServer的共享目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment">&lt;!-- 指定HBase的运行模式为分布式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment">&lt;!-- 指定Zookeeper集群的地址列表 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1,hadoop2,hadoop3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 指定HBase的主节点地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:60000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment">&lt;!-- 指定快照的存储位置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/zookeeper-3.4.9/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">   </span><br><span class="line">    <span class="comment">&lt;!-- 指定客户端连接的端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.clientPort<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>将hbase-1.2.4目录拷贝给其他节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scp –r /opt/hbase-1.2.4 slave1:/opt</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp –r /opt/hbase-1.2.4 slave2:/opt</span></span><br></pre></td></tr></table></figure><h2 id="启动HBase"><a href="#启动HBase" class="headerlink" title="启动HBase"></a>启动HBase</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /opt/hbase-1.2.4/bin</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./start-hbase.sh</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：</p><p>各个服务启动情况：hadoop-&gt;zookeeper-&gt;hbase</p><p>（1）启动Hadoop只需在主节点上执行# ./start-all.sh start</p><p>（2）启动Zookeeper需要在各个节点上执行# ./zkServer.sh start</p><p>（3）启动HBase只需在主节点上执行# ./start-hbase.sh start</p></blockquote><h2 id="查看HBase是否安装成功"><a href="#查看HBase是否安装成功" class="headerlink" title="查看HBase是否安装成功"></a>查看HBase是否安装成功</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hbase shell</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># list</span></span><br></pre></td></tr></table></figure><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p><strong>Main进程</strong></p><p>在hbase shell下，使用ctrl+z退出后查看进程会多出一个Main进程，ctrl+z退出几次，就会有几个Main进程，所以最好使用quit退出。</p>]]></content>
      
      
      <categories>
          
          <category> 大数据集群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建之Zookeeper安装</title>
      <link href="/2019/03/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BZookeeper%E5%AE%89%E8%A3%85/"/>
      <url>/2019/03/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BZookeeper%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache ZooKeeper是Apache软件基金会的一个软件项目，他为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。 ZooKeeper曾经是Hadoop的一个子项目，但现在是一个独立的顶级项目。 ZooKeeper的架构通过冗余服务实现高可用性。</p><a id="more"></a><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><table><thead><tr><th style="text-align:center">主机名</th><th style="text-align:center">IP</th><th style="text-align:center">集群规划</th></tr></thead><tbody><tr><td style="text-align:center">hadoop1</td><td style="text-align:center">192.168.159.129</td><td style="text-align:center">NameNode、DataNode、NodeManager、ResourceManager、Zookeeper</td></tr><tr><td style="text-align:center">hadoop2</td><td style="text-align:center">192.168.159.130</td><td style="text-align:center">SecondaryNameNode、DataNode、NodeManager、Zookeeper</td></tr><tr><td style="text-align:center">hadoop3</td><td style="text-align:center">192.168.159.131</td><td style="text-align:center">DataNode、NodeManager、Zookeeper</td></tr></tbody></table><h2 id="配置Zookeeper信息"><a href="#配置Zookeeper信息" class="headerlink" title="配置Zookeeper信息"></a>配置Zookeeper信息</h2><p>将zookeeper压缩包放在/home/mch目录下，并进行解压。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar -zxvf zookeeper-3.4.9.tar.gz</span></span><br></pre></td></tr></table></figure><p>配置zookeeper环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/profile</span></span><br></pre></td></tr></table></figure><p>增加如下信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export ZOOKEEPER_HOME=/home/mch/zookeeper-3.4.9</span><br><span class="line"></span><br><span class="line">export PATH=$ZOOKEEPER_HOME/bin:$PATH</span><br></pre></td></tr></table></figure><p>然后生效该文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure><p>在zookeeper-3.4.9中创建data和logs文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir data logs</span></span><br></pre></td></tr></table></figure><p>在data文件中新建myid文件，并编辑为1.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi myid</span></span><br></pre></td></tr></table></figure><p>进入到/home/mch/zookeeper-3.4.9/conf目录下，将zoo_sample.cfg重命名为zoo.cfg</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mv zoo_sample.cfg zoo.cfg</span></span><br></pre></td></tr></table></figure><p>编辑zoo.cfg信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi zoo.cfg</span></span><br></pre></td></tr></table></figure><p>修改内容如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/home/mch/zookeeper-3.4.9/data</span><br><span class="line"></span><br><span class="line">dataLogDir=/home/mch/zookeeper-3.4.9/logs</span><br><span class="line"></span><br><span class="line">server.1=192.168.159.129:2888:3888  #server.1、server.2、server.3分别对应myid下的1、2、3    </span><br><span class="line"></span><br><span class="line">server.2=192.168.159.130:2888:3888    </span><br><span class="line"></span><br><span class="line">server.3=192.168.159.131:2888:3888</span><br></pre></td></tr></table></figure><p>将hadoop1节点下的zookeeper-3.4.9目录拷贝到hadoop2和hadoop3中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scp -r zookeeper-3.4.9 hadoop2:/home/mch</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp -r zookeeper-3.4.9 hadoop3:/home/mch</span></span><br></pre></td></tr></table></figure><p>修改hadoop2和hadoop3中的myid文件内容分别为2和3。</p><h2 id="启动Zookeeper"><a href="#启动Zookeeper" class="headerlink" title="启动Zookeeper"></a>启动Zookeeper</h2><p>在三个节点上分别启动zookeeper服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/zookeeper-3.4.9/bin</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./zkServer.sh start</span></span><br></pre></td></tr></table></figure><p>查看三个节点中是否都有Zookeeper进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jps</span></span><br></pre></td></tr></table></figure><h2 id="查看zookeeper状态"><a href="#查看zookeeper状态" class="headerlink" title="查看zookeeper状态"></a>查看zookeeper状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ./zkServer.sh status</span></span><br></pre></td></tr></table></figure><p>根据paxos选举规则，可以得到一个master，两个slave</p><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p><strong>Zookeeper状态显示Error contacting service. It is probably not running.</strong></p><p>查看配置是否出错，myid是否写错。</p><p><strong>出现Error Contacting Service,It is probably not running.</strong></p><p>Zookeeper开启的台数没有大于一半，</p>]]></content>
      
      
      <categories>
          
          <category> 大数据集群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop伪分布式集群搭建</title>
      <link href="/2019/03/20/Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/03/20/Hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Apache Hadoop是一款支持数据密集型分布式应用程序并以Apache 2.0许可协议发布的开源软件框架。它支持在商品硬件构建的大型集群上运行的应用程序。Hadoop是根据谷歌公司发表的MapReduce和Google文件系统的论文自行实现而成。</p><a id="more"></a><h2 id="配置环境及规划"><a href="#配置环境及规划" class="headerlink" title="配置环境及规划"></a>配置环境及规划</h2><table><thead><tr><th style="text-align:center">主机名</th><th style="text-align:center">IP</th><th style="text-align:center">集群规划</th></tr></thead><tbody><tr><td style="text-align:center">hadoop1</td><td style="text-align:center">192.168.159.129</td><td style="text-align:center">NameNode、DataNode、NodeManager、ResourceManager</td></tr><tr><td style="text-align:center">hadoop2</td><td style="text-align:center">192.168.159.130</td><td style="text-align:center">SecondaryNameNode、DataNode、NodeManager</td></tr><tr><td style="text-align:center">hadoop3</td><td style="text-align:center">192.168.159.131</td><td style="text-align:center">DataNode、NodeManager</td></tr></tbody></table><h2 id="环境搭建及配置"><a href="#环境搭建及配置" class="headerlink" title="环境搭建及配置"></a>环境搭建及配置</h2><h3 id="Linux环境搭建"><a href="#Linux环境搭建" class="headerlink" title="Linux环境搭建"></a>Linux环境搭建</h3><p>下载VMWare Workstation，搭建Linux环境（典型），安装CentOS7系统，并命名为hadoop1。</p><blockquote><p>注意：安装镜像文件时若显示无法检测到操作系统，则选择稍后安装，下一步之后点击自定义硬件，在新CD/DVD中添加镜像文件。</p></blockquote><p>然后克隆两个虚拟机，在虚拟机-管理-克隆-创建完整克隆，并重命名hadoop2，hadoop3。</p><blockquote><p>注意：克隆的节点与主节点的ip地址是一样的，所以为了避免地址冲突，需要对克隆后的节点进行网络配置。</p></blockquote><h3 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h3><p>对三个节点进行网络配置，进入/etc/sysconfig/network-scripts目录,修改第一个形如ifcfg-ens的文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /etc/sysconfig/network-scripts</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim ifcfg-ens33</span></span><br></pre></td></tr></table></figure><p>修改并添加信息如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line">BOOTPROTO=static</span><br><span class="line"></span><br><span class="line">IPADDR=192.168.159.129 #根据网关自行调整【ip add】</span><br><span class="line"></span><br><span class="line">PREFIX=255.255.255.0  #注意需要使用PREFIX</span><br><span class="line"></span><br><span class="line">GATEWAY=192.168.159.2</span><br><span class="line"></span><br><span class="line">DNS1=114.114.114.114</span><br><span class="line"></span><br><span class="line">DNS2=202.96.134.133</span><br></pre></td></tr></table></figure><p>然后重启网络：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart network.service</span></span><br></pre></td></tr></table></figure><p>测试网络：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ping www.baidu.com</span></span><br></pre></td></tr></table></figure><p>在hadoop2和hadoop3的ifcfg-ens文件中配置其ip。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi ifcfg-ens33</span></span><br></pre></td></tr></table></figure><p>设置集群的主机名</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hostnamectl set-hostname hadoop2</span></span><br></pre></td></tr></table></figure><p>设置如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.159.129 hadoop1</span><br><span class="line"></span><br><span class="line">192.168.159.130 hadoop2</span><br><span class="line"></span><br><span class="line">192.168.159.131 hadoop3</span><br></pre></td></tr></table></figure><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><p>端口绑定了防火墙，所以需要关闭（三个节点都关闭）。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl stop firewalld.service  #关闭防火墙</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl disable firewalld.service  #禁止开机启动</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim /etc/selinux/config  #设置selinux = disabled</span></span><br></pre></td></tr></table></figure><h3 id="配置SSH免密登录"><a href="#配置SSH免密登录" class="headerlink" title="配置SSH免密登录"></a>配置SSH免密登录</h3><p>当搭建linux集群环境的时候，从一个节点登录到另一个节点，通过ssh方式，每次登录跳转都需要输入密码，这样造成非常不便，其实可以通过配置SSH互信，来实现集群节点间的免密码登录跳转。</p><p><strong>公钥认证的基本思想：</strong> 对信息的加密和解密采用不同的key，这对key分别称作private key和public key，其中，public key存放在欲登录的服务器上，而private key为特定的客户机所持有。当客户机向服务器发出建立安全连接的请求时，首先发送自己的public key，如果这个public key是被服务器所允许的，服务器就发送一个经过public key加密的随机数据给客户机，这个数据只能通过private key解密，客户机将解密后的信息发还给服务器，服务器验证正确后即确认客户机是可信任的，从而建立起一条安全的信息通道。通过这种方式，客户机不需要向外发送自己的身份标志“private key”即可达到校验的目的，并且private key是不能通过public key反向推断出来的。这避免了网络窃听可能造成的密码泄露。客户机需要小心的保存自己的private key，以免被其他人窃取，一旦这样的事情发生，就需要各服务器更换受信的public key列表。 </p><p>简言之，当我们在搭建集群过程中，在集群中机器间相互通信的时候，会不断的提示你输入通信机器的密码，然而当我们建立起集群间的ssh互信后，任意两台机器之间可以无密码的方便通信。</p><h4 id="创建公钥秘钥"><a href="#创建公钥秘钥" class="headerlink" title="创建公钥秘钥"></a>创建公钥秘钥</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir ~/.ssh  #创建.ssh目录（如果目录存在，就不必要创建）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># chmod  700 ~/.ssh  #更改.ssh权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cd ~/.ssh  #进入.ssh目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh-keygen -t rsa</span></span><br></pre></td></tr></table></figure><p>以上操作三个节点都要设置</p><h4 id="修改hosts文件"><a href="#修改hosts文件" class="headerlink" title="修改hosts文件"></a>修改hosts文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vi /etc/hosts</span></span><br></pre></td></tr></table></figure><p>添加如下信息：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">192.168.159.129 hadoop1</span><br><span class="line"></span><br><span class="line">192.168.159.130 hadoop2</span><br><span class="line"></span><br><span class="line">192.168.159.131 hadoop3</span><br></pre></td></tr></table></figure><p>以上操作只在主节点上设置</p><h4 id="整合公钥文件"><a href="#整合公钥文件" class="headerlink" title="整合公钥文件"></a>整合公钥文件</h4><p>将hadoop1中的id_rsa.pub拷贝到其他服务器中</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop3 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># chmod 600 ~/.ssh/authorized_keys</span></span><br></pre></td></tr></table></figure><h4 id="分发整合后的公钥文件"><a href="#分发整合后的公钥文件" class="headerlink" title="分发整合后的公钥文件"></a>分发整合后的公钥文件</h4><p>将整合后的公钥文件从hadoop1分别复制到hadoop2和hadoop3上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scp ~/.ssh/authorized_keys hadoop2:~/.ssh/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp ~/.ssh/authorized_keys hadoop3:~/.ssh/</span></span><br></pre></td></tr></table></figure><h3 id="分发hosts文件"><a href="#分发hosts文件" class="headerlink" title="分发hosts文件"></a>分发hosts文件</h3><p>我们需要将我们在hadoop1节点上配置的hosts文件分发到所有的节点上，在hadoop1节点上输入如下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># scp /etc/hosts hadoop2:/etc</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># scp /etc/hosts hadoop3:/etc</span></span><br></pre></td></tr></table></figure><h3 id="测试SSH互信（三个节点都设置）"><a href="#测试SSH互信（三个节点都设置）" class="headerlink" title="测试SSH互信（三个节点都设置）"></a>测试SSH互信（三个节点都设置）</h3><p>在各个节点上运行以下命令，若不需要输入密码就显示系统当前日期，就说明SSH互信已经配置成功了，在hadoop1节点上输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh hadoop2 date</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop3 date</span></span><br></pre></td></tr></table></figure><p>然后分别在hadoop2和hadoop3节点分别输入以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ssh hadoop1 date</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop3 date</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop1 date</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ssh hadoop2 date</span></span><br></pre></td></tr></table></figure><h3 id="设置时间同步"><a href="#设置时间同步" class="headerlink" title="设置时间同步"></a>设置时间同步</h3><p>利用NTP将三个节点的时间设置为同步（一个服务端，两个客户端）</p><p>如果一个集群中，时间相差很大，那么会出现很多诡异的问题。所以，设置服务器之间时间同步是必不可少的！</p><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>首先安装ntp</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install ntp</span></span><br></pre></td></tr></table></figure><p>安装完毕之后，启动服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl start ntpd.service</span></span><br></pre></td></tr></table></figure><p>设置开机自启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl enable ntpd.service</span></span><br></pre></td></tr></table></figure><p>以上操作三个节点都要设置</p><h4 id="NTP服务端设置"><a href="#NTP服务端设置" class="headerlink" title="NTP服务端设置"></a>NTP服务端设置</h4><p>第一台服务器192.168.159.129，即hadoop1，作为ntpserver，将他设置为同步外网时间（ntpd服务开启默认就同步了），但是得设置允许内网网段可以连接它，将它作为内网的时间同步服务器。</p><p>修改/etc/ntp.conf文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/ntp.conf</span></span><br></pre></td></tr></table></figure><p>添加如下信息，表示允许159网段来同步此服务器</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">restrict 192.168.159.0 mask 255.255.255.0</span><br></pre></td></tr></table></figure><p>设置后，重启ntpd服务，用ntpstat来检查效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart ntpd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ntpstat</span></span><br></pre></td></tr></table></figure><h4 id="NTP客户端设置（在hadoop2和hadoop3两个节点上设置）"><a href="#NTP客户端设置（在hadoop2和hadoop3两个节点上设置）" class="headerlink" title="NTP客户端设置（在hadoop2和hadoop3两个节点上设置）"></a>NTP客户端设置（在hadoop2和hadoop3两个节点上设置）</h4><p>第二、三台服务器192.168.159.130，192.168.159.131，即hadoop2和hadoop3，作为ntpclient，将他设置为同步上面的ntpserver，分别在两个节点上做如下配置。 </p><p>修改/etc/ntp.conf文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/ntp.conf</span></span><br></pre></td></tr></table></figure><p>注释掉外网时间服务器，添加本地服务器即可</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server 192.168.159.130  #添加此行（131服务器上则添加server 192.168.159.131）</span><br><span class="line"></span><br><span class="line"># server 0.centos.pool.ntp.org iburst  #以下四行注释掉</span><br><span class="line"></span><br><span class="line"># server 1.centos.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line"># server 2.centos.pool.ntp.org iburst</span><br><span class="line"></span><br><span class="line"># server 3.centos.pool.ntp.org iburst</span><br></pre></td></tr></table></figure><p>设置后，重启ntpd服务，用ntpstat来检查效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart ntpd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ntpstat</span></span><br></pre></td></tr></table></figure><h3 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h3><p>下载想安装的jdk版本之后，将压缩包放到/home/mch目录，解压缩后，更名为jdk1.8</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch  #进入到压缩包所在目录下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tar -xzvf jdk-8u152-linux-x64.tar.gz  #解压Linux版本的jdk解压包</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mv jdk1.8.0_111 jdk1.8  #重命名文件夹</span></span><br></pre></td></tr></table></figure><p>配置环境变量，修改/etc/profile文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>在最下方添加三行配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/mch/jdk1.8</span><br><span class="line"></span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure><p>使修改内容生效</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># source /etc/profile</span></span><br></pre></td></tr></table></figure><p>以上操作三个节点都要设置</p><h3 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h3><p>官网下载安装mysql-server</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"># rpm -ivh mysql-community-release-el7-5.noarch.rpm</span><br><span class="line"></span><br><span class="line"># yum install mysql-community-server</span><br></pre></td></tr></table></figure><p>安装成功后重启mysql服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart mysqld.service</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl enable mysqld.service  #设置mysql允许开机自启动</span></span><br></pre></td></tr></table></figure><blockquote><p>mysql只需要在主节点安装。</p></blockquote><h2 id="安装配置Hadoop2-7-2集群"><a href="#安装配置Hadoop2-7-2集群" class="headerlink" title="安装配置Hadoop2.7.2集群"></a>安装配置Hadoop2.7.2集群</h2><h3 id="安装hadoop"><a href="#安装hadoop" class="headerlink" title="安装hadoop"></a>安装hadoop</h3><p>将下载的hadoop2.6.0压缩包，上传到主节点的目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch</span></span><br></pre></td></tr></table></figure><p>进行解压缩</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tar -xzvf hadoop-2.6.0-x64.tar.gz</span></span><br></pre></td></tr></table></figure><p>修改目录为hadoop2.6.0</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mv hadoop-2.6.0 hadoop2.6.0</span></span><br></pre></td></tr></table></figure><p>修改配置文件，加入hadoop的环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vim /etc/profile</span></span><br></pre></td></tr></table></figure><p>增加内容如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/home/mch/jdk1.8</span><br><span class="line"></span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"></span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"></span><br><span class="line">export HADOOP_HOME=/home/mch/hadoop2.6.0</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br></pre></td></tr></table></figure><p>新建所需目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mkdir  /home/mch/hadoop2.6.0/tmp </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir  /home/mch/hadoop2.6.0/var  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir  /home/mch/hadoop2.6.0/dfs  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir  /home/mch/hadoop2.6.0/dfs/name  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mkdir  /home/mch/hadoop2.6.0/dfs/data</span></span><br></pre></td></tr></table></figure><h3 id="修改hadoop-env-sh文件"><a href="#修改hadoop-env-sh文件" class="headerlink" title="修改hadoop-env.sh文件"></a>修改hadoop-env.sh文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim hadoop-env.sh</span></span><br></pre></td></tr></table></figure><p>将:export JAVA_HOME=${JAVA_HOME} </p><p>修改为：export JAVA_HOME=/home/mch/jdk1.8 #修改为jdk目录</p><h3 id="修改slaves文件"><a href="#修改slaves文件" class="headerlink" title="修改slaves文件"></a>修改slaves文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim slaves</span></span><br></pre></td></tr></table></figure><p>增加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop2</span><br><span class="line"></span><br><span class="line">hadoop3</span><br></pre></td></tr></table></figure><p>此时hadoop1作为主节点，以及主备节点，管理节点，而同时hadoop1，hadoop2，hadoop3都作为数据节点。</p><h3 id="修改core-site-xml文件"><a href="#修改core-site-xml文件" class="headerlink" title="修改core-site.xml文件"></a>修改core-site.xml文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/ </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim core-site.xml</span></span><br></pre></td></tr></table></figure><p>修改内容如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop的临时目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/mch/hadoop2.6.0/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定集群中NameNode结点的URI(包括协议、主机名称、端口号) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="修改hdfs-site-xml文件"><a href="#修改hdfs-site-xml文件" class="headerlink" title="修改hdfs-site.xml文件"></a>修改hdfs-site.xml文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/ </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim hdfs-site.xml</span></span><br></pre></td></tr></table></figure><p>修改内容如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 设置NameNode的http通讯地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定SecondaryNameNode的http通讯地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop2:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode结点存储hadoop文件系统信息的本地系统路径 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/mch/hadoop2.6.0/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定DataNode结点被指定要存储数据的本地文件系统路径 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/mch/hadoop2.6.0/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定系统文件块的数据备份个数 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 关闭hadoop目录的权限 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>need not permissions<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="修改mapred-site-xml文件"><a href="#修改mapred-site-xml文件" class="headerlink" title="修改mapred-site.xml文件"></a>修改mapred-site.xml文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/ </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cp mapred-site.xml.template mapred-site.xml  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim hdfs-site.xml</span></span><br></pre></td></tr></table></figure><p>修改内容如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定JobTracker的主机和端口 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.tracker<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1:49001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定MapReduce框架为Yarn方式 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="修改yarn-site-xml文件"><a href="#修改yarn-site-xml文件" class="headerlink" title="修改yarn-site.xml文件"></a>修改yarn-site.xml文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.6.0/etc/hadoop/ </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vim yarn-site.xml</span></span><br></pre></td></tr></table></figure><p>修改内容如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager所在节点的主机名 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定ResourceManager提供给客户端访问的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>The address of the applications manager interface in the RM.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;yarn.resourcemanager.hostname&#125;:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定NodeManager执行的服务 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定向ResourceManager申请的最大内存量 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>12288<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">discription</span>&gt;</span>每个节点可用内存,单位MB,默认8182MB<span class="tag">&lt;/<span class="name">discription</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在主节点上配置好hadoop包后，同步到另外两个节点，配置不用修改，三个节点的配置都一样！</p><blockquote><p>注意：在实际应用中，应将NameNode和SecondaryNameNode配置在不同的节点上，否则当NameNode宕机时，SecondaryNameNode也会故障。</p></blockquote><h2 id="hadoop集群的初始化和启动"><a href="#hadoop集群的初始化和启动" class="headerlink" title="hadoop集群的初始化和启动"></a>hadoop集群的初始化和启动</h2><h3 id="初始化hadoop集群"><a href="#初始化hadoop集群" class="headerlink" title="初始化hadoop集群"></a>初始化hadoop集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.7.2/bin </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./hadoop namenode -format</span></span><br></pre></td></tr></table></figure><p>格式化成功后，可以在看到在/home/mch/hadoop2.7.2/dfs/name/目录多了一个current目录，而且该目录内有4个文件。</p><h3 id="启动dfs"><a href="#启动dfs" class="headerlink" title="启动dfs"></a>启动dfs</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.7.2/sbin </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./start-dfs.sh</span></span><br></pre></td></tr></table></figure><p>在三个节点上分别输入一下命令，查看三个节点上开启的进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jps</span></span><br></pre></td></tr></table></figure><p>可以看到在hadoop1中有NameNode、DataNode，在hadoop2中有SecondaryNameNode、DataNode，在hadoop3中有DataNode。</p><h3 id="启动yarn"><a href="#启动yarn" class="headerlink" title="启动yarn"></a>启动yarn</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /home/mch/hadoop2.7.2/sbin</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ./start-yarn.sh</span></span><br></pre></td></tr></table></figure><p>在三个节点上分别输入一下命令，查看三个节点上开启的进程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jps</span></span><br></pre></td></tr></table></figure><p>可以看到此时hadoop1中多了ResourceManager和NodeManager，hadoop2和hadoop3中分别多了NodeManager。</p><h2 id="集群验证"><a href="#集群验证" class="headerlink" title="集群验证"></a>集群验证</h2><p>可通过命令jps来查看集群的进程</p><p>也可通过网页来查看</p><p><a href="http://192.168.159.129:50070" target="_blank" rel="noopener">http://192.168.159.129:50070</a>          </p><p><a href="http://192.168.159.129:8088" target="_blank" rel="noopener">http://192.168.159.129:8088</a></p><h2 id="遇到的问题（不定时更新）"><a href="#遇到的问题（不定时更新）" class="headerlink" title="遇到的问题（不定时更新）"></a>遇到的问题（不定时更新）</h2><p><strong>DataNode启动失败</strong></p><p>有两个原因：</p><p>1、DataNode和NameNode的ClusterID不一致。是由于多次初始化namenode（即./hdfs namenode -format）造成的，每一次format，NameNode都会生成新的ClusterID，而DataNode还是保持原来的ClusterID。</p><p>2、将hadoop1中的配置文件拷贝到其他节点中，文件权限未开启。</p><p>原因1解决办法：</p><p>1.复制hadoop2.7.2/dfs/name/current/VERSION文件中namenode的clusterID.</p><p>2.用该clusterID把所有datanode节点机器中hadoop2.7.2/dfs/data/current/VERSION中的clusterID替换掉</p><p>原因2解决办法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># chmod 777 -R /home/mch/hadoop2.6.0</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据集群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> NameNode </tag>
            
            <tag> DataNode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ganglia监控程序安装</title>
      <link href="/2019/03/02/Ganglia%E7%9B%91%E6%8E%A7%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85/"/>
      <url>/2019/03/02/Ganglia%E7%9B%91%E6%8E%A7%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Ganglia 是 UC Berkeley 发起的一个开源监视项目，设计用于测量数以千计的节点。每台计算机都运行一个收集和发送度量数据（如处理器速度、内存使用量等）的名为 gmond 的守护进程。它将从操作系统和指定主机中收集。接收所有度量数据的主机可以显示这些数据并且可以将这些数据的精简表单传递到层次结构中。正因为有这种层次结构模式，才使得 Ganglia 可以实现良好的扩展。gmond 带来的系统负载非常少，这使得它成为在集群中各台计算机上运行的一段代码，而不会影响用户性能。</p><p>所有这些数据收集会多次影响节点性能。网络中的 “抖动（Jitter）” 发生在大量小消息同时出现时。我们发现通过将节点时钟保持一致，就可以避免这个问题。</p><a id="more"></a><h2 id="配置环境及规划"><a href="#配置环境及规划" class="headerlink" title="配置环境及规划"></a>配置环境及规划</h2><table><thead><tr><th style="text-align:center">主机名</th><th style="text-align:center">IP</th><th style="text-align:center">集群规划</th></tr></thead><tbody><tr><td style="text-align:center">hadoop1</td><td style="text-align:center">192.168.159.129</td><td style="text-align:center">NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmetad、gmond、gweb</td></tr><tr><td style="text-align:center">hadoop2</td><td style="text-align:center">192.168.159.130</td><td style="text-align:center">NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmond</td></tr><tr><td style="text-align:center">hadoop3</td><td style="text-align:center">192.168.159.131</td><td style="text-align:center">DataNode、Zookeeper、NodeManager、HRegionServer、gmond</td></tr></tbody></table><h2 id="安装过程"><a href="#安装过程" class="headerlink" title="安装过程"></a>安装过程</h2><h3 id="下载Ganglia及其依赖包"><a href="#下载Ganglia及其依赖包" class="headerlink" title="下载Ganglia及其依赖包"></a>下载Ganglia及其依赖包</h3><p>在集群的三个节点上分别下载Ganglia及其依赖包，通过yum只下载不安装，需要安装一个yum的插件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install yum-downloadonly</span></span><br></pre></td></tr></table></figure><p>安装完毕之后下载需要安装的rpm包以及依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install rrdtool httpd php ganglia* libconfuse --downloadonly --downloaddir=/home</span></span><br></pre></td></tr></table></figure><h3 id="安装Ganglia到监控端"><a href="#安装Ganglia到监控端" class="headerlink" title="安装Ganglia到监控端"></a>安装Ganglia到监控端</h3><p>在hadoop1上安装ganglia-gmetad、ganglia-web、php、httpd组件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install rrdtool httpd php ganglia-gmetad ganglia-gmond ganglia-web</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：有监控端才需要安装ganglia-metad、ganglia-web、php和httpd，其他机器不用安装</p></blockquote><h3 id="安装Ganglia到被监控端"><a href="#安装Ganglia到被监控端" class="headerlink" title="安装Ganglia到被监控端"></a>安装Ganglia到被监控端</h3><p>在hadoop2和hadoop2上安装ganglia-gmond组件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install ganglia-gmond</span></span><br></pre></td></tr></table></figure><blockquote><p>注意：被监控端只需要安装ganglia-gmond，不需要安装ganglia的其他组件。</p></blockquote><h2 id="配置过程"><a href="#配置过程" class="headerlink" title="配置过程"></a>配置过程</h2><h3 id="配置监控端的ganglia-metad和ganglia-web"><a href="#配置监控端的ganglia-metad和ganglia-web" class="headerlink" title="配置监控端的ganglia-metad和ganglia-web"></a>配置监控端的ganglia-metad和ganglia-web</h3><p>监控端端配置文件/etc/ganglia/gmetad.conf：主要是配置data_source参数。它设定了被监控端服务器的地址及端口，可以指定多个被监控端服务器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_source &quot;hadoop&quot; 15 192.168.159.129:8649 192.168.159.130:8649 192.168.159.131:8649</span><br></pre></td></tr></table></figure><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/1.png?raw=true" alt="gametad.conf"></p><p>将/usr/share/ganglia目录软链接到/var/www/html目录下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ln -s /usr/share/ganglia /var/www/html</span></span><br></pre></td></tr></table></figure><p>修改web前端配置文件/var/www/html/ganglia/conf.php，指定gmetad中存储rrd图形的目录，以及rrdtool的位置：</p><figure class="highlight php"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$gmetad_root = <span class="string">"/var/lib/ganglia"</span>;</span><br><span class="line"></span><br><span class="line">$rrds = <span class="string">"$gmetad_root/rrds"</span>;</span><br><span class="line"></span><br><span class="line">define(<span class="string">"RRDTOOL"</span>, <span class="string">"/usr/bin/rrdtool"</span>);</span><br></pre></td></tr></table></figure><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/2.png?raw=true" alt="conf.php"></p><p>修改/etc/httpd/conf.d/ganglia.conf</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/3.png?raw=true" alt="ganglia.conf"></p><h3 id="配置监控机器的ganglia-gmond"><a href="#配置监控机器的ganglia-gmond" class="headerlink" title="配置监控机器的ganglia-gmond"></a>配置监控机器的ganglia-gmond</h3><p>gmond.conf包括了几个部分：globals、cluster、udp_send_channel、udp_recv_channel等，如果只是想要Ganglia简单地运行，两个操作就可以了，两个操作都是在cluster配置段中进行修改：</p><p>修改cluster的名字，命名成“hadoop”与data_source一致</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cluster &#123;</span><br><span class="line"></span><br><span class="line">    name ="hadoop" # 和data_source中的名字一致</span><br><span class="line"></span><br><span class="line">    owner = "unspecified"</span><br><span class="line"></span><br><span class="line">    latlong = "unspecified"</span><br><span class="line"></span><br><span class="line">    url = "unspecified"</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>  <img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/4.png?raw=true" alt="gmond.conf-1"></p><p>修改接收和发送的地址，如下图所示。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">udp_send_channel &#123;</span><br><span class="line"></span><br><span class="line">    #bind_hostname = yes # Highly recommended, soon to be default.</span><br><span class="line"></span><br><span class="line">    #This option tells gmond to use a source address</span><br><span class="line"></span><br><span class="line">    #that resolves to the machine's hostname. Without</span><br><span class="line"></span><br><span class="line">    #this, the metrics may appear to come from any</span><br><span class="line"></span><br><span class="line">    #interface and the DNS names associated with</span><br><span class="line"></span><br><span class="line">    #those IPs will be used to create the RRDs.</span><br><span class="line"></span><br><span class="line">    #mcast_join = 239.2.11.71</span><br><span class="line"></span><br><span class="line">    host=192.168.159.129 # gmetad所在服务器的地址</span><br><span class="line"></span><br><span class="line">    port = 8649</span><br><span class="line"></span><br><span class="line">    ttl = 1</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">/* You can specify as many udp_recv_channels as you like as well. */</span><br><span class="line"></span><br><span class="line">udp_recv_channel &#123;</span><br><span class="line"></span><br><span class="line">    #mcast_join = 239.2.11.71</span><br><span class="line"></span><br><span class="line">    port = 8649</span><br><span class="line"></span><br><span class="line">    #bind = 239.2.11.71</span><br><span class="line"></span><br><span class="line">    retry_bind = true</span><br><span class="line"></span><br><span class="line">    #Size of the UDP buffer. If you are handling lots of metrics you really</span><br><span class="line"></span><br><span class="line">    #should bump it up to e.g. 10MB or even higher.</span><br><span class="line"></span><br><span class="line">    #buffer = 10485760</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/5.png?raw=true" alt="gmond.conf-2"></p><h3 id="配置Hadoop"><a href="#配置Hadoop" class="headerlink" title="配置Hadoop"></a>配置Hadoop</h3><p>在Hadoop中，对Ganglia的兼容是很好的，在Hadoop的目录下/hadoop-2.7.2/etc/hadoop，我们可以找到hadoop-metrics2.properties文件，这里我们修改文件内容如下所示，</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/6.png?raw=true" alt="hadoop-metrics2.properties-1"></p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/7.png?raw=true" alt="hadoop-metrics2.properties-2"></p><h3 id="配置HBase"><a href="#配置HBase" class="headerlink" title="配置HBase"></a>配置HBase</h3><p>在HBase的目录下/hbase-1.2.4/conf，我们可以找到hadoop-metrics2-hbase.properties文件，这里我们修改文件内容如下所示  </p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/8.png?raw=true" alt="hadoop-metrics2-hbase.properties"></p><p>然后将这两个文件从hadoop1复制到hadoop2和hadoop3，替换掉其原有文件。</p><h2 id="开启监控程序"><a href="#开启监控程序" class="headerlink" title="开启监控程序"></a>开启监控程序</h2><p>首先开启节点的gmond，开启的顺序为先开启发送节点的gmond，再开启其他节点的gmond，即本文先开启hadoop1的gmond，再开启hadoop2和hadoop3的gmond，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl start gmond</span></span><br></pre></td></tr></table></figure><p>然后开启主节点的gmetad，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl start gmetad</span></span><br></pre></td></tr></table></figure><p>最后开启主节点的httpd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl start httpd</span></span><br></pre></td></tr></table></figure><h2 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl status gmond -l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl status gmetad -l</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl status httpd -l</span></span><br></pre></td></tr></table></figure><p>最后通过浏览<a href="http://192.168.159.129/ganglia查看监控程序。" target="_blank" rel="noopener">http://192.168.159.129/ganglia查看监控程序。</a></p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/14.png?raw=true" alt="ganglia-web">  </p><h2 id="遇到的问题及解决办法"><a href="#遇到的问题及解决办法" class="headerlink" title="遇到的问题及解决办法"></a>遇到的问题及解决办法</h2><blockquote><p>以下有些为在其他集群运行遇到的问题。</p></blockquote><p><strong>data_thread() for [hadoop] failed to contact node …</strong></p><p>在查看gmetad状态时，出现如下错误</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/9.png?raw=true" alt="1"></p><p>是因为节点的gmond未启动，可先查看对应节点的gmond的状态，若无错误，可能是gmond的启动顺序不当造成的，可按顺序重启gmond。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># systemctl restart gmond</span></span><br></pre></td></tr></table></figure><p><strong>Error creating UDP server on port 8649 bind=…</strong></p><p>查看gmond的状态信息，若出现如下错误</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/10.png?raw=true" alt="2"></p><p>是因为在gmond.conf配置文件中，udp_recv_channel的bind地址写错了，应该将接收的bind的地址写成当前节点的地址或者直接注释掉。</p><p><strong>访问ganglia页面的时候出现Forbidden You don’t have permission to access /ganglia on this server.</strong></p><p>解决的方法：<br>1、运行命令禁用SELinux：setenforce 0（查看SELinux命令：getenforce）<br>2、运行sudo vi /etc/httpd/conf.d/ganglia.conf，将Deny from all注释掉就可以了。<br>3、重启httpd：systemctl restart httpd</p><p><strong>Failed to start The Apache HTTP Server.</strong></p><p>启动httpd失败</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/11.png?raw=true" alt="3"></p><p>原因是端口未开启，执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fuser -k -n tcp 80</span></span><br></pre></td></tr></table></figure><p>之后便可启动了。</p><p><strong>AH00558:httpd:Could not reliably determine the server’s fully qualified domain name,using … .Set the ‘ServerName’ directive globally to suppress this message.</strong></p><p>在查看http状态时出现如下信息</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/12.png?raw=true" alt="4"></p><p>在/etc/httpd/conf.d/httpd.conf中加上ServerName 192.110.10.120</p><p><strong>(code=exited, status=1/FAILURE)</strong></p><p>在查看http状态时出现如下信息</p><p><img src="https://github.com/maoconghui/markdown_photos/blob/master/ganglia/13.png?raw=true" alt="5"></p><p>将/etc/httpd/conf/httpd.conf 中的Listen 8080改为80</p>]]></content>
      
      
      <categories>
          
          <category> 大数据集群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> HBase </tag>
            
            <tag> Ganglia </tag>
            
            <tag> 监控 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
