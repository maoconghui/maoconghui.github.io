<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linux Shell笔记]]></title>
    <url>%2F2019%2F04%2F03%2FLinux%20Shell%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[最近看着视频教程学习了一下Linux Shell脚本，以防忘记，现将笔记整理如下，后续会不断完善补充。 执行文件的四种方法新建一个file1.txt文件，编辑内容为 1234echo &quot;hello world&quot;echo $$echo $sxtls -l / 其中$$表示当前目录下的进程id 执行方法： 方法一 1# source file1.txt 方法二 1# . file1.txt 方法一和方法二代表在当前进程不开启新进程的情况下读取文件并一行一行解释执行。 方法三 1# bash file1.txt 方法四 在fille1.txt文件的第一行加上#!/bin/bash，然后保存退出，给文件一个执行的权限，然后执行该文件。12# chmod +x file1.txt# ./file1.txt 方法三和方法四得到的进程id和前面的不一样，因为启动了bash，得到了新的进程id。方法三和方法四表示先创建一个新进程，然后在新进程里面读取文件并一行一行解释执行。 shell bash是解释器，执行器。解释器是用于用户交互输入和文本文件交互输入。 脚本的本质是#!/bin/bash或#!/usr/bin/python。 IO重定向输出流&gt;每个程序都有0、1、2标准输出，ls -l /显示根目录，也属于标准输出。此时要将标准输出1重定向（写到）ls.txt文件中，可使用ls -l / 1&gt;ls.txt命令。&gt;表示覆盖的重定向（会覆盖掉原文件中的内容），&gt;&gt;表示追加的重定向，&gt;和&gt;&gt;统称为重定向的操作符，0、1、2表示命令的文件描述符，0为输入，1为标准输出，,2为错误输出。注：命令的文件描述符和重定向操作符之间不允许有空格。 查看ls.txt文件。 将错误输出2重定向到ls02.txt 假设此时要将标准输出和错误输出都写到一个文件中，有两种方法。 方法一可将标准输出写到文件1中，错误输出写到文件2中，再将文件1追加重定向到文件2中。 方法二使用&gt;&amp;操作符，表示将命令的文件描述符追加重定向到另一个文件描述符中。如2&gt;&amp; 1表示将错误输出的文件描述符重定向到标准输出的文件描述符中。 重定向操作符绑定是有方向性，有先后顺序的（从左到右先后绑定）。也可不使用文件描述符，直接使用&gt;&amp; 要写入的文件名即可。 输入流&lt;read sxt 命令表示打开一个输入阻塞，等待用户的阻塞，sxt为一个变量，输入的字符串即为变量sxt的值。换行符会退出阻塞。 将一个字符串放到一个程序的输入流中 用两个输入符，首尾呼应，首尾表示开始和结束，不代表文件的内容，但是由于read命令对换行符很敏感，所以只存入换行符前面的字符，即第一行字符串SJDKLFJSLKD。 对换行符不敏感的可将read改为cat即可，也可以作为文件的内容使用source执行。 变量编辑sh01.sh脚本文件 1234567echo $#echo $*echo $@echo $1echo $2echo $11 然后执行脚本文件，并在后面加上参数1 2 3 4 5 6 7 8 9 0 a b c。 其中：$#表示传给脚本的参数个数$*表示以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个$@表示传给脚本的所有参数的列表$1表示传递给该shell脚本的第一个参数$2表示传递给该shell脚本的第二个参数$11表示$1再拼上一个1，即传递给该shell脚本的第一个参数再拼接上一个数字1，如果要取传递给该shell脚本的第十一个参数，应该将变量加上花括号。 如： 又如： 管道 | 表示管道。 sxt=100这句命令为一个父进程，在sxt=200 | echo ok这句命令中，管道两边会各触发一个子进程，所以一个子进程为sxt=200，一个子进程为echo ok，左边的子进程会重定向到右边的子进程中，即左边的输出会作为右边的输入。所以在后面执行echo $sxt的时候，执行的是父进程。即sxt=100这句。 问题1在此段命令中，为什么管道左边的子进程echo \$\$的进程id和父进程的id一样呢？这是由于\$\$的特殊性，如果换成$BASHPID则不会出现这种情况。 问题二原因是因为使用bash 命令就表示创建了一个子进程，在子进程中没有定义sxt这个变量，所以没有输出。 引用、命令替换、命令退出状态和逻辑操作符 引用单引号阻止了替换过程 命令替换反引号为命令替换 命令退出状态$?表示上一个命令的退出状态，0表示成功，其他则表示失败。 逻辑操作符 算术表达式 条件表达式、逻辑表达式 其中：-qt为大于，-qe为大于等于，-lt为小于，-le为小于等于，-eq为等于，-ne为不等于。条件表达式也可以不用test，用中括号也可。[3 -gt 2]几层逻辑最好不要用或语句，最好用上非和与。 编辑一个脚本，要实现的内容为：先判断输入的用户是否只有一个，若只有一个再判断输入的用户是否已经存在，若不存在则创建该新用户，并将密码设置为和新用户一样，整个过程静默实现，即没有其它的输出。 脚本内容如下： 流程控制语句if语句命令格式 查看根目录或god目录，若目录存在，则输出hello，反之则输出no ok。 if条件语句最后都要加上fi表示结束。 for语句 命令格式一 命令格式二 其中：NAME为变量名WORD为连续的字符，即字符串WORDS为一堆连续的字符，即用空白符分割的几个字符串 seq命令seq命令用于产生从某个数到另外一个数之间的所有整数。如： 结合for循环语句 因为此时将seq 10 当成了字符串，应该使用命令替换将命令输出的结果放在循环中。 实战实战一编辑一个脚本，实现的功能为：求出当前目录下大小最大的文件。 其中：IFS是环境变量的切割符，能够按制表符、换行符和空白符对文本进行切割，此时选择对换行符进行切割，将文件名及其大小进行分割，有分割后面就有恢复，即最后还要写上IFS=$oldIFS。 du -a为查看一个目录下的所有资源大小。 sort -n表示按数值进行排序。 sort -nr表示按数值进行倒序排序，此时选择对文件的大小进行倒序排序。 使用管道能够不将左边的结果输出。 awk为切割，此时为将文件的大小和文件名进行切割。（词的拆分） -f判断是否是一个真实存在的文件。 执行该脚本文件 在使用./findMax.sh执行的时候，会启动一个子进程去执行，所以在执行结束使用exit退出后，只是把子进程退出了，然后又回到父进程中。 在使用source findMax.sh或者. findMax.sh执行的时候，是直接在父进程中执行的，所以退出是直接将父进程退出。 实战二编辑一个脚本文件，实现的功能是：循环遍历文件每一行，输出文件每一行的内容和文件总共的行数。 方法一 方法二 wc可以计算文件的行数 方法三 使用输入流，必须放在done的后面，才可以取完一行再取下一行，假如写在read line的后面，则取的会一直是第一行。 方法四 执行结果为 行数为0 是因为使用管道的时候，是左右两边各创建一个子进程，子进程中的num为3，但是最后打印出来的num是父进程的，即为0 修改为： 也可修改为： 实战三编辑脚本文件文件如下： 执行该脚本文件，得出\$*为空白符分割显示，\$@为换行符分割显示。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群搭建之HBase安装]]></title>
    <url>%2F2019%2F03%2F22%2FHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BHBase%E5%AE%89%E8%A3%85%20%2F</url>
    <content type="text"><![CDATA[简介HBase是一个开源的非关系型分布式数据库，它参考了谷歌的BigTable建模，实现的编程语言为Java。它是Apache软件基金会的Hadoop项目的一部分，运行于HDFS文件系统之上，为 Hadoop 提供类似于BigTable 规模的服务。因此，它可以对稀疏文件提供极高的容错率。 集群规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager、Zookeeper、HMaster hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager、Zookeeper、HRegionServer hadoop3 192.168.159.131 DataNode、NodeManager、Zookeeper、HRegionServer 配置HBase信息将压缩包hbase-1.2.4.tar.gz放到/home/mch目录下，并解压 1# tar –zxvf hbase-1.2.4.tar.gz 配置hbase环境变量 1# vim /etc/profile 添加内容如下： 123export HBASE_HOME=/opt/hbase-1.2.4export PATH=$PATH:$HBASE_HOME/bin 在/hbase-1.2.4/conf目录下，修改hbase-env.sh 1# vim hbase-env.sh 修改内容如下： 1234567export JAVA_HOME=/home/mch/java/jdk1.8export HBASE_LOG_DIR=/home/mch/hbase-1.2.4/logsexport HBASE_MANAGES_ZK=falseexport HBASE_PID_DIR=/home/mch/hbase-1.2.4/pids 同时，在/home/mch/hbase-1.2.4/目录下新建logs和pids目录 1# mkdir logs pids data 在/hbase-1.2.4/conf目录下修改regionserver 1# vim regionservers 将里面的localhost删除并添加如下内容： 12345hadoop1hadoop2hadoop3 在/hbase-1.2.4/conf目录下修改hbase-site.xml 1# vim hbase-site.xml 修改内容如下： 123456789101112131415161718192021222324252627282930313233343536373839&lt;configuration&gt; &lt;!-- 指定RegionServer的共享目录 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HBase的运行模式为分布式 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Zookeeper集群的地址列表 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1,hadoop2,hadoop3&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定HBase的主节点地址 --&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:60000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定快照的存储位置 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/zookeeper-3.4.9/data&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定客户端连接的端口 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 将hbase-1.2.4目录拷贝给其他节点 123# scp –r /opt/hbase-1.2.4 slave1:/opt# scp –r /opt/hbase-1.2.4 slave2:/opt 启动HBase123# cd /opt/hbase-1.2.4/bin# ./start-hbase.sh 注意： 各个服务启动情况：hadoop-&gt;zookeeper-&gt;hbase （1）启动Hadoop只需在主节点上执行# ./start-all.sh start （2）启动Zookeeper需要在各个节点上执行# ./zkServer.sh start （3）启动HBase只需在主节点上执行# ./start-hbase.sh start 查看HBase是否安装成功123# hbase shell# list 遇到的问题 Main进程在hbase shell下，使用ctrl+z退出后查看进程会多出一个Main进程，ctrl+z退出几次，就会有几个Main进程，所以最好使用quit退出。]]></content>
      <categories>
        <category>大数据集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群搭建之Zookeeper安装]]></title>
    <url>%2F2019%2F03%2F20%2FHadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E4%B9%8BZookeeper%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[简介Apache ZooKeeper是Apache软件基金会的一个软件项目，他为大型分布式计算提供开源的分布式配置服务、同步服务和命名注册。 ZooKeeper曾经是Hadoop的一个子项目，但现在是一个独立的顶级项目。 ZooKeeper的架构通过冗余服务实现高可用性。 集群规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager、Zookeeper hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager、Zookeeper hadoop3 192.168.159.131 DataNode、NodeManager、Zookeeper 配置Zookeeper信息将zookeeper压缩包放在/home/mch目录下，并进行解压。 1# tar -zxvf zookeeper-3.4.9.tar.gz 配置zookeeper环境变量 1# vi /etc/profile 增加如下信息： 123export ZOOKEEPER_HOME=/home/mch/zookeeper-3.4.9export PATH=$ZOOKEEPER_HOME/bin:$PATH 然后生效该文件 1# source /etc/profile 在zookeeper-3.4.9中创建data和logs文件 1# mkdir data logs 在data文件中新建myid文件，并编辑为1. 1# vi myid 进入到/home/mch/zookeeper-3.4.9/conf目录下，将zoo_sample.cfg重命名为zoo.cfg 1# mv zoo_sample.cfg zoo.cfg 编辑zoo.cfg信息 1# vi zoo.cfg 修改内容如下： 123456789dataDir=/home/mch/zookeeper-3.4.9/datadataLogDir=/home/mch/zookeeper-3.4.9/logsserver.1=192.168.159.129:2888:3888 #server.1、server.2、server.3分别对应myid下的1、2、3 server.2=192.168.159.130:2888:3888 server.3=192.168.159.131:2888:3888 将hadoop1节点下的zookeeper-3.4.9目录拷贝到hadoop2和hadoop3中 123# scp -r zookeeper-3.4.9 hadoop2:/home/mch# scp -r zookeeper-3.4.9 hadoop3:/home/mch 修改hadoop2和hadoop3中的myid文件内容分别为2和3。 启动Zookeeper在三个节点上分别启动zookeeper服务 123# cd /home/mch/zookeeper-3.4.9/bin# ./zkServer.sh start 查看三个节点中是否都有Zookeeper进程 1# jps 查看zookeeper状态1# ./zkServer.sh status 根据paxos选举规则，可以得到一个master，两个slave 遇到的问题 Zookeeper状态显示Error contacting service. It is probably not running.查看配置是否出错，myid是否写错。 出现Error Contacting Service,It is probably not running.Zookeeper开启的台数没有大于一半，]]></content>
      <categories>
        <category>大数据集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop伪分布式集群搭建]]></title>
    <url>%2F2019%2F03%2F20%2FHadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[简介Apache Hadoop是一款支持数据密集型分布式应用程序并以Apache 2.0许可协议发布的开源软件框架。它支持在商品硬件构建的大型集群上运行的应用程序。Hadoop是根据谷歌公司发表的MapReduce和Google文件系统的论文自行实现而成。 配置环境及规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、DataNode、NodeManager、ResourceManager hadoop2 192.168.159.130 SecondaryNameNode、DataNode、NodeManager hadoop3 192.168.159.131 DataNode、NodeManager 环境搭建及配置Linux环境搭建下载VMWare Workstation，搭建Linux环境（典型），安装CentOS7系统，并命名为hadoop1。 注意：安装镜像文件时若显示无法检测到操作系统，则选择稍后安装，下一步之后点击自定义硬件，在新CD/DVD中添加镜像文件。 然后克隆两个虚拟机，在虚拟机-管理-克隆-创建完整克隆，并重命名hadoop2，hadoop3。 注意：克隆的节点与主节点的ip地址是一样的，所以为了避免地址冲突，需要对克隆后的节点进行网络配置。 网络配置对三个节点进行网络配置，进入/etc/sysconfig/network-scripts目录,修改第一个形如ifcfg-ens的文件。 123# cd /etc/sysconfig/network-scripts# vim ifcfg-ens33 修改并添加信息如下： 12345678910111213ONBOOT=yes BOOTPROTO=static IPADDR=192.168.159.129 #根据网关自行调整【ip add】 PREFIX=255.255.255.0 #注意需要使用PREFIX GATEWAY=192.168.159.2 DNS1=114.114.114.114 DNS2=202.96.134.133 然后重启网络： 1# systemctl restart network.service 测试网络： 1# ping www.baidu.com 在hadoop2和hadoop3的ifcfg-ens文件中配置其ip。 1# vi ifcfg-ens33 设置集群的主机名 1# hostnamectl set-hostname hadoop2 设置如下： 12345192.168.159.129 hadoop1192.168.159.130 hadoop2192.168.159.131 hadoop3 关闭防火墙端口绑定了防火墙，所以需要关闭（三个节点都关闭）。 12345# systemctl stop firewalld.service #关闭防火墙 # systemctl disable firewalld.service #禁止开机启动 # vim /etc/selinux/config #设置selinux = disabled 配置SSH免密登录当搭建linux集群环境的时候，从一个节点登录到另一个节点，通过ssh方式，每次登录跳转都需要输入密码，这样造成非常不便，其实可以通过配置SSH互信，来实现集群节点间的免密码登录跳转。 公钥认证的基本思想： 对信息的加密和解密采用不同的key，这对key分别称作private key和public key，其中，public key存放在欲登录的服务器上，而private key为特定的客户机所持有。当客户机向服务器发出建立安全连接的请求时，首先发送自己的public key，如果这个public key是被服务器所允许的，服务器就发送一个经过public key加密的随机数据给客户机，这个数据只能通过private key解密，客户机将解密后的信息发还给服务器，服务器验证正确后即确认客户机是可信任的，从而建立起一条安全的信息通道。通过这种方式，客户机不需要向外发送自己的身份标志“private key”即可达到校验的目的，并且private key是不能通过public key反向推断出来的。这避免了网络窃听可能造成的密码泄露。客户机需要小心的保存自己的private key，以免被其他人窃取，一旦这样的事情发生，就需要各服务器更换受信的public key列表。 简言之，当我们在搭建集群过程中，在集群中机器间相互通信的时候，会不断的提示你输入通信机器的密码，然而当我们建立起集群间的ssh互信后，任意两台机器之间可以无密码的方便通信。 创建公钥秘钥1234567# mkdir ~/.ssh #创建.ssh目录（如果目录存在，就不必要创建）# chmod 700 ~/.ssh #更改.ssh权限# cd ~/.ssh #进入.ssh目录# ssh-keygen -t rsa 以上操作三个节点都要设置 修改hosts文件1# vi /etc/hosts 添加如下信息： 12345192.168.159.129 hadoop1192.168.159.130 hadoop2192.168.159.131 hadoop3 以上操作只在主节点上设置 整合公钥文件将hadoop1中的id_rsa.pub拷贝到其他服务器中 1234567# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # ssh hadoop2 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# ssh hadoop3 cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys# chmod 600 ~/.ssh/authorized_keys 分发整合后的公钥文件将整合后的公钥文件从hadoop1分别复制到hadoop2和hadoop3上 123# scp ~/.ssh/authorized_keys hadoop2:~/.ssh/# scp ~/.ssh/authorized_keys hadoop3:~/.ssh/ 分发hosts文件我们需要将我们在hadoop1节点上配置的hosts文件分发到所有的节点上，在hadoop1节点上输入如下命令： 123# scp /etc/hosts hadoop2:/etc# scp /etc/hosts hadoop3:/etc 测试SSH互信（三个节点都设置）在各个节点上运行以下命令，若不需要输入密码就显示系统当前日期，就说明SSH互信已经配置成功了，在hadoop1节点上输入以下命令： 123# ssh hadoop2 date# ssh hadoop3 date 然后分别在hadoop2和hadoop3节点分别输入以下命令： 1234567# ssh hadoop1 date# ssh hadoop3 date# ssh hadoop1 date# ssh hadoop2 date 设置时间同步利用NTP将三个节点的时间设置为同步（一个服务端，两个客户端） 如果一个集群中，时间相差很大，那么会出现很多诡异的问题。所以，设置服务器之间时间同步是必不可少的！ 准备工作首先安装ntp 1# yum install ntp 安装完毕之后，启动服务 1# systemctl start ntpd.service 设置开机自启动 1# systemctl enable ntpd.service 以上操作三个节点都要设置 NTP服务端设置第一台服务器192.168.159.129，即hadoop1，作为ntpserver，将他设置为同步外网时间（ntpd服务开启默认就同步了），但是得设置允许内网网段可以连接它，将它作为内网的时间同步服务器。 修改/etc/ntp.conf文件 1# vim /etc/ntp.conf 添加如下信息，表示允许159网段来同步此服务器 1restrict 192.168.159.0 mask 255.255.255.0 设置后，重启ntpd服务，用ntpstat来检查效果 123# systemctl restart ntpd# ntpstat NTP客户端设置（在hadoop2和hadoop3两个节点上设置）第二、三台服务器192.168.159.130，192.168.159.131，即hadoop2和hadoop3，作为ntpclient，将他设置为同步上面的ntpserver，分别在两个节点上做如下配置。 修改/etc/ntp.conf文件 1# vim /etc/ntp.conf 注释掉外网时间服务器，添加本地服务器即可 123456789server 192.168.159.130 #添加此行（131服务器上则添加server 192.168.159.131）# server 0.centos.pool.ntp.org iburst #以下四行注释掉# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburst 设置后，重启ntpd服务，用ntpstat来检查效果 123# systemctl restart ntpd# ntpstat 安装jdk下载想安装的jdk版本之后，将压缩包放到/home/mch目录，解压缩后，更名为jdk1.8 12345# cd /home/mch #进入到压缩包所在目录下# tar -xzvf jdk-8u152-linux-x64.tar.gz #解压Linux版本的jdk解压包# mv jdk1.8.0_111 jdk1.8 #重命名文件夹 配置环境变量，修改/etc/profile文件 1# vim /etc/profile 在最下方添加三行配置 12345export JAVA_HOME=/home/mch/jdk1.8export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 使修改内容生效 1# source /etc/profile 以上操作三个节点都要设置 安装mysql官网下载安装mysql-server 12345# wget http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm# rpm -ivh mysql-community-release-el7-5.noarch.rpm# yum install mysql-community-server 安装成功后重启mysql服务 123# systemctl restart mysqld.service# systemctl enable mysqld.service #设置mysql允许开机自启动 mysql只需要在主节点安装。 安装配置Hadoop2.7.2集群安装hadoop将下载的hadoop2.6.0压缩包，上传到主节点的目录下 1# cd /home/mch 进行解压缩 1# tar -xzvf hadoop-2.6.0-x64.tar.gz 修改目录为hadoop2.6.0 1# mv hadoop-2.6.0 hadoop2.6.0 修改配置文件，加入hadoop的环境变量 1# vim /etc/profile 增加内容如下 123456789export JAVA_HOME=/home/mch/jdk1.8 export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin export HADOOP_HOME=/home/mch/hadoop2.6.0 export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH 新建所需目录 123456789# mkdir /home/mch/hadoop2.6.0/tmp # mkdir /home/mch/hadoop2.6.0/var # mkdir /home/mch/hadoop2.6.0/dfs # mkdir /home/mch/hadoop2.6.0/dfs/name # mkdir /home/mch/hadoop2.6.0/dfs/data 修改hadoop-env.sh文件123# cd /home/mch/hadoop2.6.0/etc/hadoop/# vim hadoop-env.sh 将:export JAVA_HOME=${JAVA_HOME} 修改为：export JAVA_HOME=/home/mch/jdk1.8 #修改为jdk目录 修改slaves文件123# cd /home/mch/hadoop2.6.0/etc/hadoop/# vim slaves 增加如下内容 123hadoop2hadoop3 此时hadoop1作为主节点，以及主备节点，管理节点，而同时hadoop1，hadoop2，hadoop3都作为数据节点。 修改core-site.xml文件123# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim core-site.xml 修改内容如下 12345678910111213141516&lt;configuration&gt; &lt;!-- 指定hadoop的临时目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/tmp&lt;/value&gt; &lt;description&gt;Abase for other temporary directories.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定集群中NameNode结点的URI(包括协议、主机名称、端口号) --&gt; &lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改hdfs-site.xml文件123# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim hdfs-site.xml 修改内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142&lt;configuration&gt; &lt;!-- 设置NameNode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;hadoop1:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定SecondaryNameNode的http通讯地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop2:50090&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode结点存储hadoop文件系统信息的本地系统路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/dfs/name&lt;/value&gt; &lt;description&gt;Path on the local filesystem where theNameNode stores the namespace and transactions logs persistently.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定DataNode结点被指定要存储数据的本地文件系统路径 --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/mch/hadoop2.6.0/dfs/data&lt;/value&gt; &lt;description&gt;Comma separated list of paths on the localfilesystem of a DataNode where it should store its blocks.&lt;/description&gt; &lt;/property&gt; &lt;!-- 指定系统文件块的数据备份个数 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 关闭hadoop目录的权限 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;need not permissions&lt;/description&gt; &lt;/property&gt; &lt;/configuration&gt; 修改mapred-site.xml文件12345# cd /home/mch/hadoop2.6.0/etc/hadoop/ # cp mapred-site.xml.template mapred-site.xml # vim hdfs-site.xml 修改内容如下 1234567891011121314&lt;configuration&gt; &lt;!-- 指定JobTracker的主机和端口 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.job.tracker&lt;/name&gt; &lt;value&gt;hadoop1:49001&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定MapReduce框架为Yarn方式 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml文件123# cd /home/mch/hadoop2.6.0/etc/hadoop/ # vim yarn-site.xml 修改内容如下 1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;!-- 指定ResourceManager所在节点的主机名 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop1&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ResourceManager提供给客户端访问的地址 --&gt; &lt;property&gt; &lt;description&gt;The address of the applications manager interface in the RM.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;$&#123;yarn.resourcemanager.hostname&#125;:8032&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NodeManager执行的服务 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定向ResourceManager申请的最大内存量 --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;12288&lt;/value&gt; &lt;discription&gt;每个节点可用内存,单位MB,默认8182MB&lt;/discription&gt; &lt;/property&gt; &lt;/configuration&gt; 在主节点上配置好hadoop包后，同步到另外两个节点，配置不用修改，三个节点的配置都一样！ 注意：在实际应用中，应将NameNode和SecondaryNameNode配置在不同的节点上，否则当NameNode宕机时，SecondaryNameNode也会故障。 hadoop集群的初始化和启动初始化hadoop集群123# cd /home/mch/hadoop2.7.2/bin # ./hadoop namenode -format 格式化成功后，可以在看到在/home/mch/hadoop2.7.2/dfs/name/目录多了一个current目录，而且该目录内有4个文件。 启动dfs123# cd /home/mch/hadoop2.7.2/sbin # ./start-dfs.sh 在三个节点上分别输入一下命令，查看三个节点上开启的进程 1# jps 可以看到在hadoop1中有NameNode、DataNode，在hadoop2中有SecondaryNameNode、DataNode，在hadoop3中有DataNode。 启动yarn123# cd /home/mch/hadoop2.7.2/sbin# ./start-yarn.sh 在三个节点上分别输入一下命令，查看三个节点上开启的进程 1# jps 可以看到此时hadoop1中多了ResourceManager和NodeManager，hadoop2和hadoop3中分别多了NodeManager。 集群验证可通过命令jps来查看集群的进程 也可通过网页来查看 http://192.168.159.129:50070 http://192.168.159.129:8088 遇到的问题（不定时更新） DataNode启动失败有两个原因：1、DataNode和NameNode的ClusterID不一致。是由于多次初始化namenode（即./hdfs namenode -format）造成的，每一次format，NameNode都会生成新的ClusterID，而DataNode还是保持原来的ClusterID。2、将hadoop1中的配置文件拷贝到其他节点中，文件权限未开启。原因1解决办法：1.复制hadoop2.7.2/dfs/name/current/VERSION文件中namenode的clusterID.2.用该clusterID把所有datanode节点机器中hadoop2.7.2/dfs/data/current/VERSION中的clusterID替换掉原因2解决办法：1# chmod 777 -R /home/mch/hadoop2.6.0]]></content>
      <categories>
        <category>大数据集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>namenode</tag>
        <tag>datanode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ganglia监控程序安装]]></title>
    <url>%2F2019%2F03%2F02%2FGanglia%E7%9B%91%E6%8E%A7%E7%A8%8B%E5%BA%8F%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[简介Ganglia 是 UC Berkeley 发起的一个开源监视项目，设计用于测量数以千计的节点。每台计算机都运行一个收集和发送度量数据（如处理器速度、内存使用量等）的名为 gmond 的守护进程。它将从操作系统和指定主机中收集。接收所有度量数据的主机可以显示这些数据并且可以将这些数据的精简表单传递到层次结构中。正因为有这种层次结构模式，才使得 Ganglia 可以实现良好的扩展。gmond 带来的系统负载非常少，这使得它成为在集群中各台计算机上运行的一段代码，而不会影响用户性能。 所有这些数据收集会多次影响节点性能。网络中的 “抖动（Jitter）” 发生在大量小消息同时出现时。我们发现通过将节点时钟保持一致，就可以避免这个问题。 配置环境及规划 主机名 IP 集群规划 hadoop1 192.168.159.129 NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmetad、gmond、gweb hadoop2 192.168.159.130 NameNode、Zookeeper、ResourceManager、HMaster、HRegionServer、gmond hadoop3 192.168.159.131 DataNode、Zookeeper、NodeManager、HRegionServer、gmond 安装过程下载Ganglia及其依赖包在集群的三个节点上分别下载Ganglia及其依赖包，通过yum只下载不安装，需要安装一个yum的插件。 1# yum install yum-downloadonly 安装完毕之后下载需要安装的rpm包以及依赖 1# yum install rrdtool httpd php ganglia* libconfuse --downloadonly --downloaddir=/home 安装Ganglia到监控端在hadoop1上安装ganglia-gmetad、ganglia-web、php、httpd组件 1# yum install rrdtool httpd php ganglia-gmetad ganglia-gmond ganglia-web 注意：有监控端才需要安装ganglia-metad、ganglia-web、php和httpd，其他机器不用安装 安装Ganglia到被监控端在hadoop2和hadoop2上安装ganglia-gmond组件 1# yum install ganglia-gmond 注意：被监控端只需要安装ganglia-gmond，不需要安装ganglia的其他组件。 配置过程配置监控端的ganglia-metad和ganglia-web监控端端配置文件/etc/ganglia/gmetad.conf：主要是配置data_source参数。它设定了被监控端服务器的地址及端口，可以指定多个被监控端服务器： 1data_source &quot;hadoop&quot; 15 192.168.159.129:8649 192.168.159.130:8649 192.168.159.131:8649 将/usr/share/ganglia目录软链接到/var/www/html目录下 1# ln -s /usr/share/ganglia /var/www/html 修改web前端配置文件/var/www/html/ganglia/conf.php，指定gmetad中存储rrd图形的目录，以及rrdtool的位置： 12345$gmetad_root = "/var/lib/ganglia";$rrds = "$gmetad_root/rrds";define("RRDTOOL", "/usr/bin/rrdtool"); 修改/etc/httpd/conf.d/ganglia.conf 配置监控机器的ganglia-gmondgmond.conf包括了几个部分：globals、cluster、udp_send_channel、udp_recv_channel等，如果只是想要Ganglia简单地运行，两个操作就可以了，两个操作都是在cluster配置段中进行修改： 修改cluster的名字，命名成“hadoop”与data_source一致 1234567891011cluster &#123; name ="hadoop" # 和data_source中的名字一致 owner = "unspecified" latlong = "unspecified" url = "unspecified"&#125; 修改接收和发送的地址，如下图所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445udp_send_channel &#123; #bind_hostname = yes # Highly recommended, soon to be default. #This option tells gmond to use a source address #that resolves to the machine's hostname. Without #this, the metrics may appear to come from any #interface and the DNS names associated with #those IPs will be used to create the RRDs. #mcast_join = 239.2.11.71 host=192.168.159.129 # gmetad所在服务器的地址 port = 8649 ttl = 1&#125; /* You can specify as many udp_recv_channels as you like as well. */udp_recv_channel &#123; #mcast_join = 239.2.11.71 port = 8649 #bind = 239.2.11.71 retry_bind = true #Size of the UDP buffer. If you are handling lots of metrics you really #should bump it up to e.g. 10MB or even higher. #buffer = 10485760&#125; 配置Hadoop在Hadoop中，对Ganglia的兼容是很好的，在Hadoop的目录下/hadoop-2.7.2/etc/hadoop，我们可以找到hadoop-metrics2.properties文件，这里我们修改文件内容如下所示， 配置HBase在HBase的目录下/hbase-1.2.4/conf，我们可以找到hadoop-metrics2-hbase.properties文件，这里我们修改文件内容如下所示 然后将这两个文件从hadoop1复制到hadoop2和hadoop3，替换掉其原有文件。 开启监控程序首先开启节点的gmond，开启的顺序为先开启发送节点的gmond，再开启其他节点的gmond，即本文先开启hadoop1的gmond，再开启hadoop2和hadoop3的gmond，命令如下： 1# systemctl start gmond 然后开启主节点的gmetad，命令如下： 1# systemctl start gmetad 最后开启主节点的httpd 1# systemctl start httpd 查看状态12345# systemctl status gmond -l# systemctl status gmetad -l# systemctl status httpd -l 最后通过浏览http://192.168.159.129/ganglia查看监控程序。 遇到的问题及解决办法 以下有些为在其他集群运行遇到的问题。 data_thread() for [hadoop] failed to contact node …在查看gmetad状态时，出现如下错误是因为节点的gmond未启动，可先查看对应节点的gmond的状态，若无错误，可能是gmond的启动顺序不当造成的，可按顺序重启gmond。 1# systemctl restart gmond Error creating UDP server on port 8649 bind=…查看gmond的状态信息，若出现如下错误是因为在gmond.conf配置文件中，udp_recv_channel的bind地址写错了，应该将接收的bind的地址写成当前节点的地址或者直接注释掉。 访问ganglia页面的时候出现Forbidden You don’t have permission to access /ganglia on this server.解决的方法：1、运行命令禁用SELinux：setenforce 0（查看SELinux命令：getenforce）2、运行sudo vi /etc/httpd/conf.d/ganglia.conf，将Deny from all注释掉就可以了。3、重启httpd：systemctl restart httpd Failed to start The Apache HTTP Server.启动httpd失败原因是端口未开启，执行1# fuser -k -n tcp 80 之后便可启动了。 AH00558:httpd:Could not reliably determine the server’s fully qualified domain name,using … .Set the ‘ServerName’ directive globally to suppress this message.在查看http状态时出现如下信息在/etc/httpd/conf.d/httpd.conf中加上ServerName 192.110.10.120 (code=exited, status=1/FAILURE)在查看http状态时出现如下信息将/etc/httpd/conf/httpd.conf 中的Listen 8080改为80]]></content>
      <categories>
        <category>大数据集群</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hbase</tag>
        <tag>ganglia</tag>
        <tag>监控</tag>
      </tags>
  </entry>
</search>
